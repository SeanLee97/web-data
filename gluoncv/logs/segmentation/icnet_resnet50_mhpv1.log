Namespace(aux=False, aux_weight=0.5, backbone='resnet50', base_size=768, batch_size=16, checkname='default', crop_size=768, ctx=[gpu(0), gpu(1), gpu(2), gpu(3), gpu(4), gpu(5), gpu(6), gpu(7), gpu(8), gpu(9), gpu(10), gpu(11), gpu(12), gpu(13), gpu(14), gpu(15)], dataset='mhpv1', dtype='float32', epochs=120, eval=False, kvstore='device', log_interval=20, logging_file='train.log', lr=1e-05, mode=None, model='icnet', model_zoo=None, momentum=0.9, ngpus=16, no_cuda=False, no_val=False, no_wd=False, norm_kwargs={'num_devices': 16}, norm_layer=<class 'mxnet.gluon.contrib.nn.basic_layers.SyncBatchNorm'>, optimizer='adam', pretrained=False, resume=None, save_dir='runs/mhpv1/icnet/resnet50/', start_epoch=0, syncbn=True, test_batch_size=16, train_split='train', warmup_epochs=0, weight_decay=0.0001, workers=48)
ICNet(
  (conv1): HybridSequential(
    (0): Conv2D(3 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_syncbatchnorm0_', in_channels=64)
    (2): Activation(relu)
    (3): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (4): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_syncbatchnorm1_', in_channels=64)
    (5): Activation(relu)
    (6): Conv2D(64 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  )
  (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_syncbatchnorm2_', in_channels=128)
  (relu): Activation(relu)
  (maxpool): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False)
  (layer1): HybridSequential(
    (0): BottleneckV1b(
      (conv1): Conv2D(128 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm0_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm1_', in_channels=64)
      (relu2): Activation(relu)
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm2_', in_channels=256)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): Conv2D(128 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_down1_syncbatchnorm0_', in_channels=256)
      )
    )
    (1): BottleneckV1b(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm3_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm4_', in_channels=64)
      (relu2): Activation(relu)
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm5_', in_channels=256)
      (relu3): Activation(relu)
    )
    (2): BottleneckV1b(
      (conv1): Conv2D(256 -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm6_', in_channels=64)
      (relu1): Activation(relu)
      (conv2): Conv2D(64 -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm7_', in_channels=64)
      (relu2): Activation(relu)
      (conv3): Conv2D(64 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers1_syncbatchnorm8_', in_channels=256)
      (relu3): Activation(relu)
    )
  )
  (layer2): HybridSequential(
    (0): BottleneckV1b(
      (conv1): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm0_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm1_', in_channels=128)
      (relu2): Activation(relu)
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm2_', in_channels=512)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): Conv2D(256 -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_down2_syncbatchnorm0_', in_channels=512)
      )
    )
    (1): BottleneckV1b(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm3_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm4_', in_channels=128)
      (relu2): Activation(relu)
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm5_', in_channels=512)
      (relu3): Activation(relu)
    )
    (2): BottleneckV1b(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm6_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm7_', in_channels=128)
      (relu2): Activation(relu)
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm8_', in_channels=512)
      (relu3): Activation(relu)
    )
    (3): BottleneckV1b(
      (conv1): Conv2D(512 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm9_', in_channels=128)
      (relu1): Activation(relu)
      (conv2): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm10_', in_channels=128)
      (relu2): Activation(relu)
      (conv3): Conv2D(128 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers2_syncbatchnorm11_', in_channels=512)
      (relu3): Activation(relu)
    )
  )
  (layer3): HybridSequential(
    (0): BottleneckV1b(
      (conv1): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm0_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm1_', in_channels=256)
      (relu2): Activation(relu)
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm2_', in_channels=1024)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): Conv2D(512 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_down3_syncbatchnorm0_', in_channels=1024)
      )
    )
    (1): BottleneckV1b(
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm3_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm4_', in_channels=256)
      (relu2): Activation(relu)
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm5_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (2): BottleneckV1b(
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm6_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm7_', in_channels=256)
      (relu2): Activation(relu)
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm8_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (3): BottleneckV1b(
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm9_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm10_', in_channels=256)
      (relu2): Activation(relu)
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm11_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (4): BottleneckV1b(
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm12_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm13_', in_channels=256)
      (relu2): Activation(relu)
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm14_', in_channels=1024)
      (relu3): Activation(relu)
    )
    (5): BottleneckV1b(
      (conv1): Conv2D(1024 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm15_', in_channels=256)
      (relu1): Activation(relu)
      (conv2): Conv2D(256 -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm16_', in_channels=256)
      (relu2): Activation(relu)
      (conv3): Conv2D(256 -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers3_syncbatchnorm17_', in_channels=1024)
      (relu3): Activation(relu)
    )
  )
  (layer4): HybridSequential(
    (0): BottleneckV1b(
      (conv1): Conv2D(1024 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm0_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm1_', in_channels=512)
      (relu2): Activation(relu)
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm2_', in_channels=2048)
      (relu3): Activation(relu)
      (downsample): HybridSequential(
        (0): Conv2D(1024 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_down4_syncbatchnorm0_', in_channels=2048)
      )
    )
    (1): BottleneckV1b(
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm3_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm4_', in_channels=512)
      (relu2): Activation(relu)
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm5_', in_channels=2048)
      (relu3): Activation(relu)
    )
    (2): BottleneckV1b(
      (conv1): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm6_', in_channels=512)
      (relu1): Activation(relu)
      (conv2): Conv2D(512 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)
      (bn2): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm7_', in_channels=512)
      (relu2): Activation(relu)
      (conv3): Conv2D(512 -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_resnetv1s_layers4_syncbatchnorm8_', in_channels=2048)
      (relu3): Activation(relu)
    )
  )
  (conv_sub1): HybridSequential(
    (0): ConvBnRelu(
      (conv): Conv2D(3 -> 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_hybridsequential0_convbnrelu0_syncbatchnorm0_', in_channels=32)
      (relu): Activation(relu)
    )
    (1): ConvBnRelu(
      (conv): Conv2D(32 -> 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_hybridsequential0_convbnrelu1_syncbatchnorm0_', in_channels=32)
      (relu): Activation(relu)
    )
    (2): ConvBnRelu(
      (conv): Conv2D(32 -> 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_hybridsequential0_convbnrelu2_syncbatchnorm0_', in_channels=64)
      (relu): Activation(relu)
    )
  )
  (psp_head): _PSPHead(
    (psp): _PyramidPooling(
      (conv1): HybridSequential(
        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0__pyramidpooling0_hybridsequential0_syncbatchnorm0_', in_channels=512)
        (2): Activation(relu)
      )
      (conv2): HybridSequential(
        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0__pyramidpooling0_hybridsequential1_syncbatchnorm0_', in_channels=512)
        (2): Activation(relu)
      )
      (conv3): HybridSequential(
        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0__pyramidpooling0_hybridsequential2_syncbatchnorm0_', in_channels=512)
        (2): Activation(relu)
      )
      (conv4): HybridSequential(
        (0): Conv2D(2048 -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0__pyramidpooling0_hybridsequential3_syncbatchnorm0_', in_channels=512)
        (2): Activation(relu)
      )
    )
    (block): HybridSequential(
      (0): Conv2D(4096 -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0__psphead0_syncbatchnorm0_', in_channels=512)
      (2): Activation(relu)
      (3): Dropout(p = 0.1, axes=())
    )
  )
  (head): _ICHead(
    (cff_12): CascadeFeatureFusion(
      (conv_low): HybridSequential(
        (0): Conv2D(128 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_cascadefeaturefusion0_hybridsequential0_syncbatchnorm0_', in_channels=128)
      )
      (conv_hign): HybridSequential(
        (0): Conv2D(64 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_cascadefeaturefusion0_hybridsequential1_syncbatchnorm0_', in_channels=128)
      )
      (conv_low_cls): Conv2D(128 -> 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (cff_24): CascadeFeatureFusion(
      (conv_low): HybridSequential(
        (0): Conv2D(256 -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_cascadefeaturefusion1_hybridsequential0_syncbatchnorm0_', in_channels=128)
      )
      (conv_hign): HybridSequential(
        (0): Conv2D(256 -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_cascadefeaturefusion1_hybridsequential1_syncbatchnorm0_', in_channels=128)
      )
      (conv_low_cls): Conv2D(128 -> 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
    )
    (conv_cls): Conv2D(128 -> 19, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
  (conv_sub4): ConvBnRelu(
    (conv): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_convbnrelu0_syncbatchnorm0_', in_channels=256)
    (relu): Activation(relu)
  )
  (conv_sub2): ConvBnRelu(
    (conv): Conv2D(512 -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): SyncBatchNorm(eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=False, ndev=16, key='icnet0_convbnrelu1_syncbatchnorm0_', in_channels=256)
    (relu): Activation(relu)
  )
)
Starting Epoch: 0
Total Epochs: 120
Epoch 0 iteration 0020/0187: training loss 7.274
Epoch 0 iteration 0040/0187: training loss 6.591
Epoch 0 iteration 0060/0187: training loss 6.114
Epoch 0 iteration 0080/0187: training loss 5.736
Epoch 0 iteration 0100/0188: training loss 5.413
Epoch 0 iteration 0120/0188: training loss 5.142
Epoch 0 iteration 0140/0188: training loss 4.910
Epoch 0 iteration 0160/0188: training loss 4.706
Epoch 0 iteration 0180/0188: training loss 4.520
Epoch 0 validation pixAcc: 0.776, mIoU: 0.165
Epoch 1 iteration 0020/0187: training loss 2.912
Epoch 1 iteration 0040/0187: training loss 2.821
Epoch 1 iteration 0060/0187: training loss 2.733
Epoch 1 iteration 0080/0187: training loss 2.658
Epoch 1 iteration 0100/0187: training loss 2.604
Epoch 1 iteration 0120/0187: training loss 2.546
Epoch 1 iteration 0140/0187: training loss 2.507
Epoch 1 iteration 0160/0187: training loss 2.474
Epoch 1 iteration 0180/0187: training loss 2.440
Epoch 1 validation pixAcc: 0.814, mIoU: 0.223
Epoch 2 iteration 0020/0187: training loss 2.013
Epoch 2 iteration 0040/0187: training loss 2.015
Epoch 2 iteration 0060/0187: training loss 1.997
Epoch 2 iteration 0080/0187: training loss 1.971
Epoch 2 iteration 0100/0188: training loss 1.944
Epoch 2 iteration 0120/0188: training loss 1.923
Epoch 2 iteration 0140/0188: training loss 1.911
Epoch 2 iteration 0160/0188: training loss 1.896
Epoch 2 iteration 0180/0188: training loss 1.883
Epoch 2 validation pixAcc: 0.830, mIoU: 0.246
Epoch 3 iteration 0020/0187: training loss 1.783
Epoch 3 iteration 0040/0187: training loss 1.749
Epoch 3 iteration 0060/0187: training loss 1.708
Epoch 3 iteration 0080/0187: training loss 1.675
Epoch 3 iteration 0100/0187: training loss 1.677
Epoch 3 iteration 0120/0187: training loss 1.662
Epoch 3 iteration 0140/0187: training loss 1.647
Epoch 3 iteration 0160/0187: training loss 1.627
Epoch 3 iteration 0180/0187: training loss 1.615
Epoch 3 validation pixAcc: 0.839, mIoU: 0.260
Epoch 4 iteration 0020/0187: training loss 1.513
Epoch 4 iteration 0040/0187: training loss 1.517
Epoch 4 iteration 0060/0187: training loss 1.511
Epoch 4 iteration 0080/0187: training loss 1.525
Epoch 4 iteration 0100/0188: training loss 1.528
Epoch 4 iteration 0120/0188: training loss 1.506
Epoch 4 iteration 0140/0188: training loss 1.496
Epoch 4 iteration 0160/0188: training loss 1.490
Epoch 4 iteration 0180/0188: training loss 1.483
Epoch 4 validation pixAcc: 0.845, mIoU: 0.285
Epoch 5 iteration 0020/0187: training loss 1.342
Epoch 5 iteration 0040/0187: training loss 1.390
Epoch 5 iteration 0060/0187: training loss 1.416
Epoch 5 iteration 0080/0187: training loss 1.405
Epoch 5 iteration 0100/0187: training loss 1.395
Epoch 5 iteration 0120/0187: training loss 1.386
Epoch 5 iteration 0140/0187: training loss 1.378
Epoch 5 iteration 0160/0187: training loss 1.366
Epoch 5 iteration 0180/0187: training loss 1.368
Epoch 5 validation pixAcc: 0.851, mIoU: 0.297
Epoch 6 iteration 0020/0187: training loss 1.310
Epoch 6 iteration 0040/0187: training loss 1.317
Epoch 6 iteration 0060/0187: training loss 1.303
Epoch 6 iteration 0080/0187: training loss 1.293
Epoch 6 iteration 0100/0188: training loss 1.283
Epoch 6 iteration 0120/0188: training loss 1.278
Epoch 6 iteration 0140/0188: training loss 1.271
Epoch 6 iteration 0160/0188: training loss 1.263
Epoch 6 iteration 0180/0188: training loss 1.261
Epoch 6 validation pixAcc: 0.850, mIoU: 0.304
Epoch 7 iteration 0020/0187: training loss 1.194
Epoch 7 iteration 0040/0187: training loss 1.208
Epoch 7 iteration 0060/0187: training loss 1.187
Epoch 7 iteration 0080/0187: training loss 1.194
Epoch 7 iteration 0100/0187: training loss 1.194
Epoch 7 iteration 0120/0187: training loss 1.194
Epoch 7 iteration 0140/0187: training loss 1.195
Epoch 7 iteration 0160/0187: training loss 1.199
Epoch 7 iteration 0180/0187: training loss 1.195
Epoch 7 validation pixAcc: 0.855, mIoU: 0.319
Epoch 8 iteration 0020/0187: training loss 1.194
Epoch 8 iteration 0040/0187: training loss 1.176
Epoch 8 iteration 0060/0187: training loss 1.172
Epoch 8 iteration 0080/0187: training loss 1.159
Epoch 8 iteration 0100/0188: training loss 1.152
Epoch 8 iteration 0120/0188: training loss 1.142
Epoch 8 iteration 0140/0188: training loss 1.137
Epoch 8 iteration 0160/0188: training loss 1.132
Epoch 8 iteration 0180/0188: training loss 1.133
Epoch 8 validation pixAcc: 0.857, mIoU: 0.331
Epoch 9 iteration 0020/0187: training loss 1.093
Epoch 9 iteration 0040/0187: training loss 1.122
Epoch 9 iteration 0060/0187: training loss 1.097
Epoch 9 iteration 0080/0187: training loss 1.085
Epoch 9 iteration 0100/0187: training loss 1.093
Epoch 9 iteration 0120/0187: training loss 1.093
Epoch 9 iteration 0140/0187: training loss 1.097
Epoch 9 iteration 0160/0187: training loss 1.099
Epoch 9 iteration 0180/0187: training loss 1.106
Epoch 9 validation pixAcc: 0.860, mIoU: 0.342
Epoch 10 iteration 0020/0187: training loss 1.073
Epoch 10 iteration 0040/0187: training loss 1.063
Epoch 10 iteration 0060/0187: training loss 1.050
Epoch 10 iteration 0080/0187: training loss 1.051
Epoch 10 iteration 0100/0188: training loss 1.041
Epoch 10 iteration 0120/0188: training loss 1.044
Epoch 10 iteration 0140/0188: training loss 1.046
Epoch 10 iteration 0160/0188: training loss 1.046
Epoch 10 iteration 0180/0188: training loss 1.046
Epoch 10 validation pixAcc: 0.862, mIoU: 0.350
Epoch 11 iteration 0020/0187: training loss 1.080
Epoch 11 iteration 0040/0187: training loss 1.072
Epoch 11 iteration 0060/0187: training loss 1.058
Epoch 11 iteration 0080/0187: training loss 1.036
Epoch 11 iteration 0100/0187: training loss 1.028
Epoch 11 iteration 0120/0187: training loss nan
Epoch 11 iteration 0140/0187: training loss nan
Epoch 11 iteration 0160/0187: training loss nan
Epoch 11 iteration 0180/0187: training loss nan
Epoch 11 validation pixAcc: 0.860, mIoU: 0.353
Epoch 12 iteration 0020/0187: training loss nan
Epoch 12 iteration 0040/0187: training loss nan
Epoch 12 iteration 0060/0187: training loss nan
Epoch 12 iteration 0080/0187: training loss nan
Epoch 12 iteration 0100/0188: training loss nan
Epoch 12 iteration 0120/0188: training loss nan
Epoch 12 iteration 0140/0188: training loss nan
Epoch 12 iteration 0160/0188: training loss nan
Epoch 12 iteration 0180/0188: training loss nan
Epoch 12 validation pixAcc: 0.864, mIoU: 0.355
Epoch 13 iteration 0020/0187: training loss nan
Epoch 13 iteration 0040/0187: training loss nan
Epoch 13 iteration 0060/0187: training loss nan
Epoch 13 iteration 0080/0187: training loss nan
Epoch 13 iteration 0100/0187: training loss nan
Epoch 13 iteration 0120/0187: training loss nan
Epoch 13 iteration 0140/0187: training loss nan
Epoch 13 iteration 0160/0187: training loss nan
Epoch 13 iteration 0180/0187: training loss nan
Epoch 13 validation pixAcc: 0.865, mIoU: 0.359
Epoch 14 iteration 0020/0187: training loss 0.955
Epoch 14 iteration 0040/0187: training loss nan
Epoch 14 iteration 0060/0187: training loss nan
Epoch 14 iteration 0080/0187: training loss nan
Epoch 14 iteration 0100/0188: training loss nan
Epoch 14 iteration 0120/0188: training loss nan
Epoch 14 iteration 0140/0188: training loss nan
Epoch 14 iteration 0160/0188: training loss nan
Epoch 14 iteration 0180/0188: training loss nan
Epoch 14 validation pixAcc: 0.866, mIoU: 0.367
Epoch 15 iteration 0020/0187: training loss 0.949
Epoch 15 iteration 0040/0187: training loss 0.916
Epoch 15 iteration 0060/0187: training loss 0.941
Epoch 15 iteration 0080/0187: training loss 0.948
Epoch 15 iteration 0100/0187: training loss 0.940
Epoch 15 iteration 0120/0187: training loss nan
Epoch 15 iteration 0140/0187: training loss nan
Epoch 15 iteration 0160/0187: training loss nan
Epoch 15 iteration 0180/0187: training loss nan
Epoch 15 validation pixAcc: 0.867, mIoU: 0.362
Epoch 16 iteration 0020/0187: training loss 0.858
Epoch 16 iteration 0040/0187: training loss nan
Epoch 16 iteration 0060/0187: training loss nan
Epoch 16 iteration 0080/0187: training loss nan
Epoch 16 iteration 0100/0188: training loss nan
Epoch 16 iteration 0120/0188: training loss nan
Epoch 16 iteration 0140/0188: training loss nan
Epoch 16 iteration 0160/0188: training loss nan
Epoch 16 iteration 0180/0188: training loss nan
Epoch 16 validation pixAcc: 0.870, mIoU: 0.370
Epoch 17 iteration 0020/0187: training loss nan
Epoch 17 iteration 0040/0187: training loss nan
Epoch 17 iteration 0060/0187: training loss nan
Epoch 17 iteration 0080/0187: training loss nan
Epoch 17 iteration 0100/0187: training loss nan
Epoch 17 iteration 0120/0187: training loss nan
Epoch 17 iteration 0140/0187: training loss nan
Epoch 17 iteration 0160/0187: training loss nan
Epoch 17 iteration 0180/0187: training loss nan
Epoch 17 validation pixAcc: 0.870, mIoU: 0.369
Epoch 18 iteration 0020/0187: training loss nan
Epoch 18 iteration 0040/0187: training loss nan
Epoch 18 iteration 0060/0187: training loss nan
Epoch 18 iteration 0080/0187: training loss nan
Epoch 18 iteration 0100/0188: training loss nan
Epoch 18 iteration 0120/0188: training loss nan
Epoch 18 iteration 0140/0188: training loss nan
Epoch 18 iteration 0160/0188: training loss nan
Epoch 18 iteration 0180/0188: training loss nan
Epoch 18 validation pixAcc: 0.868, mIoU: 0.372
Epoch 19 iteration 0020/0187: training loss 0.811
Epoch 19 iteration 0040/0187: training loss 0.838
Epoch 19 iteration 0060/0187: training loss 0.841
Epoch 19 iteration 0080/0187: training loss nan
Epoch 19 iteration 0100/0187: training loss nan
Epoch 19 iteration 0120/0187: training loss nan
Epoch 19 iteration 0140/0187: training loss nan
Epoch 19 iteration 0160/0187: training loss nan
Epoch 19 iteration 0180/0187: training loss nan
Epoch 19 validation pixAcc: 0.870, mIoU: 0.374
Epoch 20 iteration 0020/0187: training loss 0.815
Epoch 20 iteration 0040/0187: training loss nan
Epoch 20 iteration 0060/0187: training loss nan
Epoch 20 iteration 0080/0187: training loss nan
Epoch 20 iteration 0100/0188: training loss nan
Epoch 20 iteration 0120/0188: training loss nan
Epoch 20 iteration 0140/0188: training loss nan
Epoch 20 iteration 0160/0188: training loss nan
Epoch 20 iteration 0180/0188: training loss nan
Epoch 20 validation pixAcc: 0.869, mIoU: 0.375
Epoch 21 iteration 0020/0187: training loss nan
Epoch 21 iteration 0040/0187: training loss nan
Epoch 21 iteration 0060/0187: training loss nan
Epoch 21 iteration 0080/0187: training loss nan
Epoch 21 iteration 0100/0187: training loss nan
Epoch 21 iteration 0120/0187: training loss nan
Epoch 21 iteration 0140/0187: training loss nan
Epoch 21 iteration 0160/0187: training loss nan
Epoch 21 iteration 0180/0187: training loss nan
Epoch 21 validation pixAcc: 0.872, mIoU: 0.375
Epoch 22 iteration 0020/0187: training loss nan
Epoch 22 iteration 0040/0187: training loss nan
Epoch 22 iteration 0060/0187: training loss nan
Epoch 22 iteration 0080/0187: training loss nan
Epoch 22 iteration 0100/0188: training loss nan
Epoch 22 iteration 0120/0188: training loss nan
Epoch 22 iteration 0140/0188: training loss nan
Epoch 22 iteration 0160/0188: training loss nan
Epoch 22 iteration 0180/0188: training loss nan
Epoch 22 validation pixAcc: 0.871, mIoU: 0.375
Epoch 23 iteration 0020/0187: training loss 0.793
Epoch 23 iteration 0040/0187: training loss nan
Epoch 23 iteration 0060/0187: training loss nan
Epoch 23 iteration 0080/0187: training loss nan
Epoch 23 iteration 0100/0187: training loss nan
Epoch 23 iteration 0120/0187: training loss nan
Epoch 23 iteration 0140/0187: training loss nan
Epoch 23 iteration 0160/0187: training loss nan
Epoch 23 iteration 0180/0187: training loss nan
Epoch 23 validation pixAcc: 0.872, mIoU: 0.381
Epoch 24 iteration 0020/0187: training loss 0.832
Epoch 24 iteration 0040/0187: training loss nan
Epoch 24 iteration 0060/0187: training loss nan
Epoch 24 iteration 0080/0187: training loss nan
Epoch 24 iteration 0100/0188: training loss nan
Epoch 24 iteration 0120/0188: training loss nan
Epoch 24 iteration 0140/0188: training loss nan
Epoch 24 iteration 0160/0188: training loss nan
Epoch 24 iteration 0180/0188: training loss nan
Epoch 24 validation pixAcc: 0.872, mIoU: 0.379
Epoch 25 iteration 0020/0187: training loss 0.754
Epoch 25 iteration 0040/0187: training loss nan
Epoch 25 iteration 0060/0187: training loss nan
Epoch 25 iteration 0080/0187: training loss nan
Epoch 25 iteration 0100/0187: training loss nan
Epoch 25 iteration 0120/0187: training loss nan
Epoch 25 iteration 0140/0187: training loss nan
Epoch 25 iteration 0160/0187: training loss nan
Epoch 25 iteration 0180/0187: training loss nan
Epoch 25 validation pixAcc: 0.872, mIoU: 0.380
Epoch 26 iteration 0020/0187: training loss 0.718
Epoch 26 iteration 0040/0187: training loss nan
Epoch 26 iteration 0060/0187: training loss nan
Epoch 26 iteration 0080/0187: training loss nan
Epoch 26 iteration 0100/0188: training loss nan
Epoch 26 iteration 0120/0188: training loss nan
Epoch 26 iteration 0140/0188: training loss nan
Epoch 26 iteration 0160/0188: training loss nan
Epoch 26 iteration 0180/0188: training loss nan
Epoch 26 validation pixAcc: 0.872, mIoU: 0.381
Epoch 27 iteration 0020/0187: training loss 0.760
Epoch 27 iteration 0040/0187: training loss nan
Epoch 27 iteration 0060/0187: training loss nan
Epoch 27 iteration 0080/0187: training loss nan
Epoch 27 iteration 0100/0187: training loss nan
Epoch 27 iteration 0120/0187: training loss nan
Epoch 27 iteration 0140/0187: training loss nan
Epoch 27 iteration 0160/0187: training loss nan
Epoch 27 iteration 0180/0187: training loss nan
Epoch 27 validation pixAcc: 0.872, mIoU: 0.384
Epoch 28 iteration 0020/0187: training loss 0.730
Epoch 28 iteration 0040/0187: training loss nan
Epoch 28 iteration 0060/0187: training loss nan
Epoch 28 iteration 0080/0187: training loss nan
Epoch 28 iteration 0100/0188: training loss nan
Epoch 28 iteration 0120/0188: training loss nan
Epoch 28 iteration 0140/0188: training loss nan
Epoch 28 iteration 0160/0188: training loss nan
Epoch 28 iteration 0180/0188: training loss nan
Epoch 28 validation pixAcc: 0.873, mIoU: 0.391
Epoch 29 iteration 0020/0187: training loss 0.695
Epoch 29 iteration 0040/0187: training loss nan
Epoch 29 iteration 0060/0187: training loss nan
Epoch 29 iteration 0080/0187: training loss nan
Epoch 29 iteration 0100/0187: training loss nan
Epoch 29 iteration 0120/0187: training loss nan
Epoch 29 iteration 0140/0187: training loss nan
Epoch 29 iteration 0160/0187: training loss nan
Epoch 29 iteration 0180/0187: training loss nan
Epoch 29 validation pixAcc: 0.872, mIoU: 0.386
Epoch 30 iteration 0020/0187: training loss 0.729
Epoch 30 iteration 0040/0187: training loss nan
Epoch 30 iteration 0060/0187: training loss nan
Epoch 30 iteration 0080/0187: training loss nan
Epoch 30 iteration 0100/0188: training loss nan
Epoch 30 iteration 0120/0188: training loss nan
Epoch 30 iteration 0140/0188: training loss nan
Epoch 30 iteration 0160/0188: training loss nan
Epoch 30 iteration 0180/0188: training loss nan
Epoch 30 validation pixAcc: 0.875, mIoU: 0.390
Epoch 31 iteration 0020/0187: training loss 0.710
Epoch 31 iteration 0040/0187: training loss nan
Epoch 31 iteration 0060/0187: training loss nan
Epoch 31 iteration 0080/0187: training loss nan
Epoch 31 iteration 0100/0187: training loss nan
Epoch 31 iteration 0120/0187: training loss nan
Epoch 31 iteration 0140/0187: training loss nan
Epoch 31 iteration 0160/0187: training loss nan
Epoch 31 iteration 0180/0187: training loss nan
Epoch 31 validation pixAcc: 0.876, mIoU: 0.393
Epoch 32 iteration 0020/0187: training loss 0.717
Epoch 32 iteration 0040/0187: training loss 0.706
Epoch 32 iteration 0060/0187: training loss 0.690
Epoch 32 iteration 0080/0187: training loss nan
Epoch 32 iteration 0100/0188: training loss nan
Epoch 32 iteration 0120/0188: training loss nan
Epoch 32 iteration 0140/0188: training loss nan
Epoch 32 iteration 0160/0188: training loss nan
Epoch 32 iteration 0180/0188: training loss nan
Epoch 32 validation pixAcc: 0.876, mIoU: 0.397
Epoch 33 iteration 0020/0187: training loss 0.644
Epoch 33 iteration 0040/0187: training loss nan
Epoch 33 iteration 0060/0187: training loss nan
Epoch 33 iteration 0080/0187: training loss nan
Epoch 33 iteration 0100/0187: training loss nan
Epoch 33 iteration 0120/0187: training loss nan
Epoch 33 iteration 0140/0187: training loss nan
Epoch 33 iteration 0160/0187: training loss nan
Epoch 33 iteration 0180/0187: training loss nan
Epoch 33 validation pixAcc: 0.876, mIoU: 0.399
Epoch 34 iteration 0020/0187: training loss 0.688
Epoch 34 iteration 0040/0187: training loss 0.687
Epoch 34 iteration 0060/0187: training loss nan
Epoch 34 iteration 0080/0187: training loss nan
Epoch 34 iteration 0100/0188: training loss nan
Epoch 34 iteration 0120/0188: training loss nan
Epoch 34 iteration 0140/0188: training loss nan
Epoch 34 iteration 0160/0188: training loss nan
Epoch 34 iteration 0180/0188: training loss nan
Epoch 34 validation pixAcc: 0.876, mIoU: 0.401
Epoch 35 iteration 0020/0187: training loss 0.679
Epoch 35 iteration 0040/0187: training loss 0.673
Epoch 35 iteration 0060/0187: training loss 0.667
Epoch 35 iteration 0080/0187: training loss nan
Epoch 35 iteration 0100/0187: training loss nan
Epoch 35 iteration 0120/0187: training loss nan
Epoch 35 iteration 0140/0187: training loss nan
Epoch 35 iteration 0160/0187: training loss nan
Epoch 35 iteration 0180/0187: training loss nan
Epoch 35 validation pixAcc: 0.878, mIoU: 0.403
Epoch 36 iteration 0020/0187: training loss nan
Epoch 36 iteration 0040/0187: training loss nan
Epoch 36 iteration 0060/0187: training loss nan
Epoch 36 iteration 0080/0187: training loss nan
Epoch 36 iteration 0100/0188: training loss nan
Epoch 36 iteration 0120/0188: training loss nan
Epoch 36 iteration 0140/0188: training loss nan
Epoch 36 iteration 0160/0188: training loss nan
Epoch 36 iteration 0180/0188: training loss nan
Epoch 36 validation pixAcc: 0.877, mIoU: 0.409
Epoch 37 iteration 0020/0187: training loss 0.632
Epoch 37 iteration 0040/0187: training loss 0.632
Epoch 37 iteration 0060/0187: training loss 0.651
Epoch 37 iteration 0080/0187: training loss 0.655
Epoch 37 iteration 0100/0187: training loss 0.659
Epoch 37 iteration 0120/0187: training loss 0.654
Epoch 37 iteration 0140/0187: training loss nan
Epoch 37 iteration 0160/0187: training loss nan
Epoch 37 iteration 0180/0187: training loss nan
Epoch 37 validation pixAcc: 0.878, mIoU: 0.407
Epoch 38 iteration 0020/0187: training loss 0.658
Epoch 38 iteration 0040/0187: training loss 0.656
Epoch 38 iteration 0060/0187: training loss 0.642
Epoch 38 iteration 0080/0187: training loss 0.642
Epoch 38 iteration 0100/0188: training loss 0.641
Epoch 38 iteration 0120/0188: training loss nan
Epoch 38 iteration 0140/0188: training loss nan
Epoch 38 iteration 0160/0188: training loss nan
Epoch 38 iteration 0180/0188: training loss nan
Epoch 38 validation pixAcc: 0.878, mIoU: 0.407
Epoch 39 iteration 0020/0187: training loss 0.644
Epoch 39 iteration 0040/0187: training loss 0.646
Epoch 39 iteration 0060/0187: training loss 0.647
Epoch 39 iteration 0080/0187: training loss nan
Epoch 39 iteration 0100/0187: training loss nan
Epoch 39 iteration 0120/0187: training loss nan
Epoch 39 iteration 0140/0187: training loss nan
Epoch 39 iteration 0160/0187: training loss nan
Epoch 39 iteration 0180/0187: training loss nan
Epoch 39 validation pixAcc: 0.879, mIoU: 0.408
Epoch 40 iteration 0020/0187: training loss 0.644
Epoch 40 iteration 0040/0187: training loss 0.639
Epoch 40 iteration 0060/0187: training loss nan
Epoch 40 iteration 0080/0187: training loss nan
Epoch 40 iteration 0100/0188: training loss nan
Epoch 40 iteration 0120/0188: training loss nan
Epoch 40 iteration 0140/0188: training loss nan
Epoch 40 iteration 0160/0188: training loss nan
Epoch 40 iteration 0180/0188: training loss nan
Epoch 40 validation pixAcc: 0.877, mIoU: 0.398
Epoch 41 iteration 0020/0187: training loss 0.628
Epoch 41 iteration 0040/0187: training loss nan
Epoch 41 iteration 0060/0187: training loss nan
Epoch 41 iteration 0080/0187: training loss nan
Epoch 41 iteration 0100/0187: training loss nan
Epoch 41 iteration 0120/0187: training loss nan
Epoch 41 iteration 0140/0187: training loss nan
Epoch 41 iteration 0160/0187: training loss nan
Epoch 41 iteration 0180/0187: training loss nan
Epoch 41 validation pixAcc: 0.878, mIoU: 0.406
Epoch 42 iteration 0020/0187: training loss 0.633
Epoch 42 iteration 0040/0187: training loss nan
Epoch 42 iteration 0060/0187: training loss nan
Epoch 42 iteration 0080/0187: training loss nan
Epoch 42 iteration 0100/0188: training loss nan
Epoch 42 iteration 0120/0188: training loss nan
Epoch 42 iteration 0140/0188: training loss nan
Epoch 42 iteration 0160/0188: training loss nan
Epoch 42 iteration 0180/0188: training loss nan
Epoch 42 validation pixAcc: 0.880, mIoU: 0.409
Epoch 43 iteration 0020/0187: training loss 0.612
Epoch 43 iteration 0040/0187: training loss 0.615
Epoch 43 iteration 0060/0187: training loss 0.619
Epoch 43 iteration 0080/0187: training loss 0.617
Epoch 43 iteration 0100/0187: training loss 0.616
Epoch 43 iteration 0120/0187: training loss 0.621
Epoch 43 iteration 0140/0187: training loss nan
Epoch 43 iteration 0160/0187: training loss nan
Epoch 43 iteration 0180/0187: training loss nan
Epoch 43 validation pixAcc: 0.879, mIoU: 0.415
Epoch 44 iteration 0020/0187: training loss 0.637
Epoch 44 iteration 0040/0187: training loss 0.630
Epoch 44 iteration 0060/0187: training loss 0.637
Epoch 44 iteration 0080/0187: training loss 0.625
Epoch 44 iteration 0100/0188: training loss nan
Epoch 44 iteration 0120/0188: training loss nan
Epoch 44 iteration 0140/0188: training loss nan
Epoch 44 iteration 0160/0188: training loss nan
Epoch 44 iteration 0180/0188: training loss nan
Epoch 44 validation pixAcc: 0.880, mIoU: 0.416
Epoch 45 iteration 0020/0187: training loss 0.621
Epoch 45 iteration 0040/0187: training loss 0.594
Epoch 45 iteration 0060/0187: training loss 0.596
Epoch 45 iteration 0080/0187: training loss 0.595
Epoch 45 iteration 0100/0187: training loss 0.606
Epoch 45 iteration 0120/0187: training loss 0.610
Epoch 45 iteration 0140/0187: training loss 0.608
Epoch 45 iteration 0160/0187: training loss 0.610
Epoch 45 iteration 0180/0187: training loss 0.610
Epoch 45 validation pixAcc: 0.879, mIoU: 0.419
Epoch 46 iteration 0020/0187: training loss 0.584
Epoch 46 iteration 0040/0187: training loss 0.591
Epoch 46 iteration 0060/0187: training loss nan
Epoch 46 iteration 0080/0187: training loss nan
Epoch 46 iteration 0100/0188: training loss nan
Epoch 46 iteration 0120/0188: training loss nan
Epoch 46 iteration 0140/0188: training loss nan
Epoch 46 iteration 0160/0188: training loss nan
Epoch 46 iteration 0180/0188: training loss nan
Epoch 46 validation pixAcc: 0.880, mIoU: 0.419
Epoch 47 iteration 0020/0187: training loss 0.586
Epoch 47 iteration 0040/0187: training loss 0.600
Epoch 47 iteration 0060/0187: training loss 0.616
Epoch 47 iteration 0080/0187: training loss nan
Epoch 47 iteration 0100/0187: training loss nan
Epoch 47 iteration 0120/0187: training loss nan
Epoch 47 iteration 0140/0187: training loss nan
Epoch 47 iteration 0160/0187: training loss nan
Epoch 47 iteration 0180/0187: training loss nan
Epoch 47 validation pixAcc: 0.880, mIoU: 0.417
Epoch 48 iteration 0020/0187: training loss 0.597
Epoch 48 iteration 0040/0187: training loss 0.599
Epoch 48 iteration 0060/0187: training loss 0.600
Epoch 48 iteration 0080/0187: training loss 0.603
Epoch 48 iteration 0100/0188: training loss 0.608
Epoch 48 iteration 0120/0188: training loss 0.604
Epoch 48 iteration 0140/0188: training loss 0.604
Epoch 48 iteration 0160/0188: training loss nan
Epoch 48 iteration 0180/0188: training loss nan
Epoch 48 validation pixAcc: 0.880, mIoU: 0.416
Epoch 49 iteration 0020/0187: training loss 0.628
Epoch 49 iteration 0040/0187: training loss 0.627
Epoch 49 iteration 0060/0187: training loss 0.624
Epoch 49 iteration 0080/0187: training loss 0.610
Epoch 49 iteration 0100/0187: training loss 0.607
Epoch 49 iteration 0120/0187: training loss 0.601
Epoch 49 iteration 0140/0187: training loss 0.599
Epoch 49 iteration 0160/0187: training loss 0.598
Epoch 49 iteration 0180/0187: training loss 0.599
Epoch 49 validation pixAcc: 0.879, mIoU: 0.420
Epoch 50 iteration 0020/0187: training loss 0.654
Epoch 50 iteration 0040/0187: training loss 0.612
Epoch 50 iteration 0060/0187: training loss 0.609
Epoch 50 iteration 0080/0187: training loss 0.607
Epoch 50 iteration 0100/0188: training loss 0.608
Epoch 50 iteration 0120/0188: training loss 0.602
Epoch 50 iteration 0140/0188: training loss 0.597
Epoch 50 iteration 0160/0188: training loss 0.598
Epoch 50 iteration 0180/0188: training loss nan
Epoch 50 validation pixAcc: 0.880, mIoU: 0.424
Epoch 51 iteration 0020/0187: training loss 0.621
Epoch 51 iteration 0040/0187: training loss 0.605
Epoch 51 iteration 0060/0187: training loss 0.599
Epoch 51 iteration 0080/0187: training loss 0.597
Epoch 51 iteration 0100/0187: training loss 0.596
Epoch 51 iteration 0120/0187: training loss 0.595
Epoch 51 iteration 0140/0187: training loss nan
Epoch 51 iteration 0160/0187: training loss nan
Epoch 51 iteration 0180/0187: training loss nan
Epoch 51 validation pixAcc: 0.880, mIoU: 0.424
Epoch 52 iteration 0020/0187: training loss 0.575
Epoch 52 iteration 0040/0187: training loss 0.586
Epoch 52 iteration 0060/0187: training loss 0.583
Epoch 52 iteration 0080/0187: training loss 0.581
Epoch 52 iteration 0100/0188: training loss 0.581
Epoch 52 iteration 0120/0188: training loss 0.579
Epoch 52 iteration 0140/0188: training loss 0.581
Epoch 52 iteration 0160/0188: training loss 0.584
Epoch 52 iteration 0180/0188: training loss 0.581
Epoch 52 validation pixAcc: 0.881, mIoU: 0.421
Epoch 53 iteration 0020/0187: training loss 0.607
Epoch 53 iteration 0040/0187: training loss 0.584
Epoch 53 iteration 0060/0187: training loss 0.580
Epoch 53 iteration 0080/0187: training loss 0.580
Epoch 53 iteration 0100/0187: training loss 0.588
Epoch 53 iteration 0120/0187: training loss 0.585
Epoch 53 iteration 0140/0187: training loss 0.586
Epoch 53 iteration 0160/0187: training loss 0.587
Epoch 53 iteration 0180/0187: training loss 0.589
Epoch 53 validation pixAcc: 0.881, mIoU: 0.420
Epoch 54 iteration 0020/0187: training loss 0.585
Epoch 54 iteration 0040/0187: training loss 0.568
Epoch 54 iteration 0060/0187: training loss 0.574
Epoch 54 iteration 0080/0187: training loss 0.584
Epoch 54 iteration 0100/0188: training loss 0.581
Epoch 54 iteration 0120/0188: training loss 0.581
Epoch 54 iteration 0140/0188: training loss nan
Epoch 54 iteration 0160/0188: training loss nan
Epoch 54 iteration 0180/0188: training loss nan
Epoch 54 validation pixAcc: 0.882, mIoU: 0.429
Epoch 55 iteration 0020/0187: training loss 0.563
Epoch 55 iteration 0040/0187: training loss 0.565
Epoch 55 iteration 0060/0187: training loss 0.569
Epoch 55 iteration 0080/0187: training loss 0.573
Epoch 55 iteration 0100/0187: training loss 0.571
Epoch 55 iteration 0120/0187: training loss 0.570
Epoch 55 iteration 0140/0187: training loss 0.571
Epoch 55 iteration 0160/0187: training loss 0.570
Epoch 55 iteration 0180/0187: training loss 0.572
Epoch 55 validation pixAcc: 0.882, mIoU: 0.421
Epoch 56 iteration 0020/0187: training loss 0.579
Epoch 56 iteration 0040/0187: training loss 0.569
Epoch 56 iteration 0060/0187: training loss 0.577
Epoch 56 iteration 0080/0187: training loss 0.579
Epoch 56 iteration 0100/0188: training loss 0.580
Epoch 56 iteration 0120/0188: training loss 0.572
Epoch 56 iteration 0140/0188: training loss 0.569
Epoch 56 iteration 0160/0188: training loss 0.573
Epoch 56 iteration 0180/0188: training loss 0.572
Epoch 56 validation pixAcc: 0.882, mIoU: 0.420
Epoch 57 iteration 0020/0187: training loss 0.573
Epoch 57 iteration 0040/0187: training loss 0.570
Epoch 57 iteration 0060/0187: training loss 0.572
Epoch 57 iteration 0080/0187: training loss 0.561
Epoch 57 iteration 0100/0187: training loss 0.567
Epoch 57 iteration 0120/0187: training loss 0.566
Epoch 57 iteration 0140/0187: training loss 0.568
Epoch 57 iteration 0160/0187: training loss 0.572
Epoch 57 iteration 0180/0187: training loss 0.570
Epoch 57 validation pixAcc: 0.882, mIoU: 0.423
Epoch 58 iteration 0020/0187: training loss 0.563
Epoch 58 iteration 0040/0187: training loss 0.556
Epoch 58 iteration 0060/0187: training loss 0.565
Epoch 58 iteration 0080/0187: training loss 0.569
Epoch 58 iteration 0100/0188: training loss 0.570
Epoch 58 iteration 0120/0188: training loss 0.576
Epoch 58 iteration 0140/0188: training loss 0.573
Epoch 58 iteration 0160/0188: training loss 0.567
Epoch 58 iteration 0180/0188: training loss 0.565
Epoch 58 validation pixAcc: 0.882, mIoU: 0.417
Epoch 59 iteration 0020/0187: training loss 0.573
Epoch 59 iteration 0040/0187: training loss 0.568
Epoch 59 iteration 0060/0187: training loss 0.569
Epoch 59 iteration 0080/0187: training loss 0.569
Epoch 59 iteration 0100/0187: training loss 0.562
Epoch 59 iteration 0120/0187: training loss 0.562
Epoch 59 iteration 0140/0187: training loss 0.566
Epoch 59 iteration 0160/0187: training loss 0.564
Epoch 59 iteration 0180/0187: training loss 0.567
Epoch 59 validation pixAcc: 0.880, mIoU: 0.419
Epoch 60 iteration 0020/0187: training loss 0.587
Epoch 60 iteration 0040/0187: training loss 0.583
Epoch 60 iteration 0060/0187: training loss 0.592
Epoch 60 iteration 0080/0187: training loss 0.581
Epoch 60 iteration 0100/0188: training loss 0.572
Epoch 60 iteration 0120/0188: training loss 0.568
Epoch 60 iteration 0140/0188: training loss 0.569
Epoch 60 iteration 0160/0188: training loss 0.567
Epoch 60 iteration 0180/0188: training loss 0.565
Epoch 60 validation pixAcc: 0.881, mIoU: 0.419
Epoch 61 iteration 0020/0187: training loss 0.530
Epoch 61 iteration 0040/0187: training loss 0.547
Epoch 61 iteration 0060/0187: training loss 0.553
Epoch 61 iteration 0080/0187: training loss 0.557
Epoch 61 iteration 0100/0187: training loss 0.553
Epoch 61 iteration 0120/0187: training loss 0.550
Epoch 61 iteration 0140/0187: training loss 0.553
Epoch 61 iteration 0160/0187: training loss 0.554
Epoch 61 iteration 0180/0187: training loss 0.555
Epoch 61 validation pixAcc: 0.880, mIoU: 0.426
Epoch 62 iteration 0020/0187: training loss 0.568
Epoch 62 iteration 0040/0187: training loss 0.544
Epoch 62 iteration 0060/0187: training loss 0.557
Epoch 62 iteration 0080/0187: training loss 0.559
Epoch 62 iteration 0100/0188: training loss 0.555
Epoch 62 iteration 0120/0188: training loss 0.555
Epoch 62 iteration 0140/0188: training loss 0.559
Epoch 62 iteration 0160/0188: training loss 0.557
Epoch 62 iteration 0180/0188: training loss 0.556
Epoch 62 validation pixAcc: 0.882, mIoU: 0.422
Epoch 63 iteration 0020/0187: training loss 0.578
Epoch 63 iteration 0040/0187: training loss 0.567
Epoch 63 iteration 0060/0187: training loss 0.574
Epoch 63 iteration 0080/0187: training loss 0.567
Epoch 63 iteration 0100/0187: training loss 0.568
Epoch 63 iteration 0120/0187: training loss 0.568
Epoch 63 iteration 0140/0187: training loss 0.563
Epoch 63 iteration 0160/0187: training loss 0.558
Epoch 63 iteration 0180/0187: training loss 0.557
Epoch 63 validation pixAcc: 0.883, mIoU: 0.428
Epoch 64 iteration 0020/0187: training loss 0.537
Epoch 64 iteration 0040/0187: training loss 0.551
Epoch 64 iteration 0060/0187: training loss 0.547
Epoch 64 iteration 0080/0187: training loss 0.553
Epoch 64 iteration 0100/0188: training loss 0.544
Epoch 64 iteration 0120/0188: training loss 0.547
Epoch 64 iteration 0140/0188: training loss 0.546
Epoch 64 iteration 0160/0188: training loss 0.550
Epoch 64 iteration 0180/0188: training loss 0.550
Epoch 64 validation pixAcc: 0.881, mIoU: 0.423
Epoch 65 iteration 0020/0187: training loss 0.522
Epoch 65 iteration 0040/0187: training loss 0.540
Epoch 65 iteration 0060/0187: training loss 0.543
Epoch 65 iteration 0080/0187: training loss 0.541
Epoch 65 iteration 0100/0187: training loss 0.538
Epoch 65 iteration 0120/0187: training loss 0.541
Epoch 65 iteration 0140/0187: training loss 0.540
Epoch 65 iteration 0160/0187: training loss 0.544
Epoch 65 iteration 0180/0187: training loss 0.543
Epoch 65 validation pixAcc: 0.881, mIoU: 0.426
Epoch 66 iteration 0020/0187: training loss 0.516
Epoch 66 iteration 0040/0187: training loss 0.531
Epoch 66 iteration 0060/0187: training loss 0.544
Epoch 66 iteration 0080/0187: training loss 0.549
Epoch 66 iteration 0100/0188: training loss 0.545
Epoch 66 iteration 0120/0188: training loss 0.544
Epoch 66 iteration 0140/0188: training loss 0.544
Epoch 66 iteration 0160/0188: training loss 0.544
Epoch 66 iteration 0180/0188: training loss 0.545
Epoch 66 validation pixAcc: 0.880, mIoU: 0.420
Epoch 67 iteration 0020/0187: training loss 0.527
Epoch 67 iteration 0040/0187: training loss 0.544
Epoch 67 iteration 0060/0187: training loss 0.543
Epoch 67 iteration 0080/0187: training loss 0.537
Epoch 67 iteration 0100/0187: training loss 0.540
Epoch 67 iteration 0120/0187: training loss 0.540
Epoch 67 iteration 0140/0187: training loss 0.537
Epoch 67 iteration 0160/0187: training loss 0.536
Epoch 67 iteration 0180/0187: training loss 0.536
Epoch 67 validation pixAcc: 0.883, mIoU: 0.423
Epoch 68 iteration 0020/0187: training loss 0.531
Epoch 68 iteration 0040/0187: training loss 0.532
Epoch 68 iteration 0060/0187: training loss 0.549
Epoch 68 iteration 0080/0187: training loss 0.554
Epoch 68 iteration 0100/0188: training loss 0.549
Epoch 68 iteration 0120/0188: training loss 0.547
Epoch 68 iteration 0140/0188: training loss 0.547
Epoch 68 iteration 0160/0188: training loss 0.546
Epoch 68 iteration 0180/0188: training loss 0.545
Epoch 68 validation pixAcc: 0.882, mIoU: 0.423
Epoch 69 iteration 0020/0187: training loss 0.536
Epoch 69 iteration 0040/0187: training loss 0.539
Epoch 69 iteration 0060/0187: training loss 0.533
Epoch 69 iteration 0080/0187: training loss 0.541
Epoch 69 iteration 0100/0187: training loss 0.545
Epoch 69 iteration 0120/0187: training loss 0.539
Epoch 69 iteration 0140/0187: training loss 0.542
Epoch 69 iteration 0160/0187: training loss 0.542
Epoch 69 iteration 0180/0187: training loss 0.543
Epoch 69 validation pixAcc: 0.882, mIoU: 0.423
Epoch 70 iteration 0020/0187: training loss 0.540
Epoch 70 iteration 0040/0187: training loss 0.538
Epoch 70 iteration 0060/0187: training loss 0.545
Epoch 70 iteration 0080/0187: training loss 0.542
Epoch 70 iteration 0100/0188: training loss 0.547
Epoch 70 iteration 0120/0188: training loss 0.547
Epoch 70 iteration 0140/0188: training loss 0.540
Epoch 70 iteration 0160/0188: training loss 0.539
Epoch 70 iteration 0180/0188: training loss 0.536
Epoch 70 validation pixAcc: 0.881, mIoU: 0.417
Epoch 71 iteration 0020/0187: training loss 0.549
Epoch 71 iteration 0040/0187: training loss 0.550
Epoch 71 iteration 0060/0187: training loss 0.540
Epoch 71 iteration 0080/0187: training loss 0.535
Epoch 71 iteration 0100/0187: training loss 0.534
Epoch 71 iteration 0120/0187: training loss 0.532
Epoch 71 iteration 0140/0187: training loss 0.532
Epoch 71 iteration 0160/0187: training loss 0.534
Epoch 71 iteration 0180/0187: training loss 0.534
Epoch 71 validation pixAcc: 0.882, mIoU: 0.420
Epoch 72 iteration 0020/0187: training loss 0.550
Epoch 72 iteration 0040/0187: training loss 0.532
Epoch 72 iteration 0060/0187: training loss 0.530
Epoch 72 iteration 0080/0187: training loss 0.531
Epoch 72 iteration 0100/0188: training loss 0.528
Epoch 72 iteration 0120/0188: training loss 0.528
Epoch 72 iteration 0140/0188: training loss 0.530
Epoch 72 iteration 0160/0188: training loss 0.531
Epoch 72 iteration 0180/0188: training loss 0.532
Epoch 72 validation pixAcc: 0.883, mIoU: 0.421
Epoch 73 iteration 0020/0187: training loss 0.529
Epoch 73 iteration 0040/0187: training loss 0.513
Epoch 73 iteration 0060/0187: training loss 0.529
Epoch 73 iteration 0080/0187: training loss 0.526
Epoch 73 iteration 0100/0187: training loss 0.525
Epoch 73 iteration 0120/0187: training loss 0.524
Epoch 73 iteration 0140/0187: training loss 0.526
Epoch 73 iteration 0160/0187: training loss 0.527
Epoch 73 iteration 0180/0187: training loss 0.530
Epoch 73 validation pixAcc: 0.883, mIoU: 0.426
Epoch 74 iteration 0020/0187: training loss 0.551
Epoch 74 iteration 0040/0187: training loss 0.532
Epoch 74 iteration 0060/0187: training loss 0.526
Epoch 74 iteration 0080/0187: training loss 0.521
Epoch 74 iteration 0100/0188: training loss 0.519
Epoch 74 iteration 0120/0188: training loss 0.523
Epoch 74 iteration 0140/0188: training loss 0.525
Epoch 74 iteration 0160/0188: training loss 0.522
Epoch 74 iteration 0180/0188: training loss 0.522
Epoch 74 validation pixAcc: 0.884, mIoU: 0.424
Epoch 75 iteration 0020/0187: training loss 0.534
Epoch 75 iteration 0040/0187: training loss 0.528
Epoch 75 iteration 0060/0187: training loss 0.523
Epoch 75 iteration 0080/0187: training loss 0.519
Epoch 75 iteration 0100/0187: training loss 0.520
Epoch 75 iteration 0120/0187: training loss 0.519
Epoch 75 iteration 0140/0187: training loss 0.521
Epoch 75 iteration 0160/0187: training loss 0.520
Epoch 75 iteration 0180/0187: training loss 0.521
Epoch 75 validation pixAcc: 0.882, mIoU: 0.425
Epoch 76 iteration 0020/0187: training loss 0.514
Epoch 76 iteration 0040/0187: training loss 0.530
Epoch 76 iteration 0060/0187: training loss 0.531
Epoch 76 iteration 0080/0187: training loss 0.534
Epoch 76 iteration 0100/0188: training loss 0.535
Epoch 76 iteration 0120/0188: training loss 0.529
Epoch 76 iteration 0140/0188: training loss 0.522
Epoch 76 iteration 0160/0188: training loss 0.521
Epoch 76 iteration 0180/0188: training loss 0.521
Epoch 76 validation pixAcc: 0.885, mIoU: 0.428
Epoch 77 iteration 0020/0187: training loss 0.489
Epoch 77 iteration 0040/0187: training loss 0.497
Epoch 77 iteration 0060/0187: training loss 0.504
Epoch 77 iteration 0080/0187: training loss 0.504
Epoch 77 iteration 0100/0187: training loss 0.511
Epoch 77 iteration 0120/0187: training loss 0.513
Epoch 77 iteration 0140/0187: training loss 0.519
Epoch 77 iteration 0160/0187: training loss 0.519
Epoch 77 iteration 0180/0187: training loss 0.520
Epoch 77 validation pixAcc: 0.883, mIoU: 0.427
Epoch 78 iteration 0020/0187: training loss 0.510
Epoch 78 iteration 0040/0187: training loss 0.520
Epoch 78 iteration 0060/0187: training loss 0.516
Epoch 78 iteration 0080/0187: training loss 0.517
Epoch 78 iteration 0100/0188: training loss 0.514
Epoch 78 iteration 0120/0188: training loss 0.513
Epoch 78 iteration 0140/0188: training loss 0.516
Epoch 78 iteration 0160/0188: training loss 0.517
Epoch 78 iteration 0180/0188: training loss 0.517
Epoch 78 validation pixAcc: 0.883, mIoU: 0.427
Epoch 79 iteration 0020/0187: training loss 0.514
Epoch 79 iteration 0040/0187: training loss 0.517
Epoch 79 iteration 0060/0187: training loss 0.525
Epoch 79 iteration 0080/0187: training loss 0.526
Epoch 79 iteration 0100/0187: training loss 0.525
Epoch 79 iteration 0120/0187: training loss 0.521
Epoch 79 iteration 0140/0187: training loss 0.518
Epoch 79 iteration 0160/0187: training loss 0.519
Epoch 79 iteration 0180/0187: training loss 0.515
Epoch 79 validation pixAcc: 0.884, mIoU: 0.428
Epoch 80 iteration 0020/0187: training loss 0.521
Epoch 80 iteration 0040/0187: training loss 0.514
Epoch 80 iteration 0060/0187: training loss 0.506
Epoch 80 iteration 0080/0187: training loss 0.498
Epoch 80 iteration 0100/0188: training loss 0.505
Epoch 80 iteration 0120/0188: training loss 0.505
Epoch 80 iteration 0140/0188: training loss 0.505
Epoch 80 iteration 0160/0188: training loss 0.506
Epoch 80 iteration 0180/0188: training loss 0.509
Epoch 80 validation pixAcc: 0.885, mIoU: 0.432
Epoch 81 iteration 0020/0187: training loss 0.531
Epoch 81 iteration 0040/0187: training loss 0.524
Epoch 81 iteration 0060/0187: training loss 0.510
Epoch 81 iteration 0080/0187: training loss 0.510
Epoch 81 iteration 0100/0187: training loss 0.515
Epoch 81 iteration 0120/0187: training loss 0.513
Epoch 81 iteration 0140/0187: training loss 0.515
Epoch 81 iteration 0160/0187: training loss 0.513
Epoch 81 iteration 0180/0187: training loss 0.514
Epoch 81 validation pixAcc: 0.884, mIoU: 0.424
Epoch 82 iteration 0020/0187: training loss 0.510
Epoch 82 iteration 0040/0187: training loss 0.506
Epoch 82 iteration 0060/0187: training loss 0.495
Epoch 82 iteration 0080/0187: training loss 0.500
Epoch 82 iteration 0100/0188: training loss 0.499
Epoch 82 iteration 0120/0188: training loss 0.502
Epoch 82 iteration 0140/0188: training loss 0.505
Epoch 82 iteration 0160/0188: training loss 0.508
Epoch 82 iteration 0180/0188: training loss 0.508
Epoch 82 validation pixAcc: 0.884, mIoU: 0.425
Epoch 83 iteration 0020/0187: training loss 0.474
Epoch 83 iteration 0040/0187: training loss 0.493
Epoch 83 iteration 0060/0187: training loss 0.496
Epoch 83 iteration 0080/0187: training loss 0.500
Epoch 83 iteration 0100/0187: training loss 0.500
Epoch 83 iteration 0120/0187: training loss 0.507
Epoch 83 iteration 0140/0187: training loss 0.509
Epoch 83 iteration 0160/0187: training loss 0.511
Epoch 83 iteration 0180/0187: training loss 0.511
Epoch 83 validation pixAcc: 0.885, mIoU: 0.430
Epoch 84 iteration 0020/0187: training loss 0.498
Epoch 84 iteration 0040/0187: training loss 0.499
Epoch 84 iteration 0060/0187: training loss 0.513
Epoch 84 iteration 0080/0187: training loss 0.509
Epoch 84 iteration 0100/0188: training loss 0.504
Epoch 84 iteration 0120/0188: training loss 0.503
Epoch 84 iteration 0140/0188: training loss 0.505
Epoch 84 iteration 0160/0188: training loss 0.506
Epoch 84 iteration 0180/0188: training loss 0.506
Epoch 84 validation pixAcc: 0.884, mIoU: 0.427
Epoch 85 iteration 0020/0187: training loss 0.509
Epoch 85 iteration 0040/0187: training loss 0.516
Epoch 85 iteration 0060/0187: training loss 0.511
Epoch 85 iteration 0080/0187: training loss 0.507
Epoch 85 iteration 0100/0187: training loss 0.502
Epoch 85 iteration 0120/0187: training loss 0.500
Epoch 85 iteration 0140/0187: training loss 0.500
Epoch 85 iteration 0160/0187: training loss 0.500
Epoch 85 iteration 0180/0187: training loss 0.499
Epoch 85 validation pixAcc: 0.885, mIoU: 0.429
Epoch 86 iteration 0020/0187: training loss 0.491
Epoch 86 iteration 0040/0187: training loss 0.488
Epoch 86 iteration 0060/0187: training loss 0.489
Epoch 86 iteration 0080/0187: training loss 0.492
Epoch 86 iteration 0100/0188: training loss 0.491
Epoch 86 iteration 0120/0188: training loss 0.497
Epoch 86 iteration 0140/0188: training loss 0.502
Epoch 86 iteration 0160/0188: training loss 0.506
Epoch 86 iteration 0180/0188: training loss 0.503
Epoch 86 validation pixAcc: 0.884, mIoU: 0.429
Epoch 87 iteration 0020/0187: training loss 0.514
Epoch 87 iteration 0040/0187: training loss 0.503
Epoch 87 iteration 0060/0187: training loss 0.503
Epoch 87 iteration 0080/0187: training loss 0.504
Epoch 87 iteration 0100/0187: training loss 0.502
Epoch 87 iteration 0120/0187: training loss 0.501
Epoch 87 iteration 0140/0187: training loss 0.501
Epoch 87 iteration 0160/0187: training loss 0.502
Epoch 87 iteration 0180/0187: training loss 0.503
Epoch 87 validation pixAcc: 0.884, mIoU: 0.428
Epoch 88 iteration 0020/0187: training loss 0.489
Epoch 88 iteration 0040/0187: training loss 0.492
Epoch 88 iteration 0060/0187: training loss 0.497
Epoch 88 iteration 0080/0187: training loss 0.501
Epoch 88 iteration 0100/0188: training loss 0.497
Epoch 88 iteration 0120/0188: training loss 0.499
Epoch 88 iteration 0140/0188: training loss 0.498
Epoch 88 iteration 0160/0188: training loss 0.499
Epoch 88 iteration 0180/0188: training loss 0.502
Epoch 88 validation pixAcc: 0.885, mIoU: 0.426
Epoch 89 iteration 0020/0187: training loss 0.517
Epoch 89 iteration 0040/0187: training loss 0.508
Epoch 89 iteration 0060/0187: training loss 0.504
Epoch 89 iteration 0080/0187: training loss 0.505
Epoch 89 iteration 0100/0187: training loss 0.509
Epoch 89 iteration 0120/0187: training loss 0.507
Epoch 89 iteration 0140/0187: training loss 0.506
Epoch 89 iteration 0160/0187: training loss 0.508
Epoch 89 iteration 0180/0187: training loss 0.507
Epoch 89 validation pixAcc: 0.883, mIoU: 0.426
Epoch 90 iteration 0020/0187: training loss 0.494
Epoch 90 iteration 0040/0187: training loss 0.495
Epoch 90 iteration 0060/0187: training loss 0.497
Epoch 90 iteration 0080/0187: training loss 0.495
Epoch 90 iteration 0100/0188: training loss 0.499
Epoch 90 iteration 0120/0188: training loss 0.495
Epoch 90 iteration 0140/0188: training loss 0.496
Epoch 90 iteration 0160/0188: training loss 0.495
Epoch 90 iteration 0180/0188: training loss 0.495
Epoch 90 validation pixAcc: 0.885, mIoU: 0.431
Epoch 91 iteration 0020/0187: training loss 0.487
Epoch 91 iteration 0040/0187: training loss 0.493
Epoch 91 iteration 0060/0187: training loss 0.491
Epoch 91 iteration 0080/0187: training loss 0.487
Epoch 91 iteration 0100/0187: training loss 0.482
Epoch 91 iteration 0120/0187: training loss 0.485
Epoch 91 iteration 0140/0187: training loss 0.488
Epoch 91 iteration 0160/0187: training loss 0.489
Epoch 91 iteration 0180/0187: training loss 0.488
Epoch 91 validation pixAcc: 0.883, mIoU: 0.425
Epoch 92 iteration 0020/0187: training loss 0.520
Epoch 92 iteration 0040/0187: training loss 0.513
Epoch 92 iteration 0060/0187: training loss 0.503
Epoch 92 iteration 0080/0187: training loss 0.492
Epoch 92 iteration 0100/0188: training loss 0.492
Epoch 92 iteration 0120/0188: training loss 0.492
Epoch 92 iteration 0140/0188: training loss 0.489
Epoch 92 iteration 0160/0188: training loss 0.489
Epoch 92 iteration 0180/0188: training loss 0.490
Epoch 92 validation pixAcc: 0.885, mIoU: 0.432
Epoch 93 iteration 0020/0187: training loss 0.491
Epoch 93 iteration 0040/0187: training loss 0.502
Epoch 93 iteration 0060/0187: training loss 0.501
Epoch 93 iteration 0080/0187: training loss 0.491
Epoch 93 iteration 0100/0187: training loss 0.484
Epoch 93 iteration 0120/0187: training loss 0.488
Epoch 93 iteration 0140/0187: training loss 0.489
Epoch 93 iteration 0160/0187: training loss 0.491
Epoch 93 iteration 0180/0187: training loss 0.494
Epoch 93 validation pixAcc: 0.884, mIoU: 0.434
Epoch 94 iteration 0020/0187: training loss 0.473
Epoch 94 iteration 0040/0187: training loss 0.472
Epoch 94 iteration 0060/0187: training loss 0.467
Epoch 94 iteration 0080/0187: training loss 0.476
Epoch 94 iteration 0100/0188: training loss 0.477
Epoch 94 iteration 0120/0188: training loss 0.480
Epoch 94 iteration 0140/0188: training loss 0.484
Epoch 94 iteration 0160/0188: training loss 0.489
Epoch 94 iteration 0180/0188: training loss 0.489
Epoch 94 validation pixAcc: 0.884, mIoU: 0.429
Epoch 95 iteration 0020/0187: training loss 0.489
Epoch 95 iteration 0040/0187: training loss 0.490
Epoch 95 iteration 0060/0187: training loss 0.487
Epoch 95 iteration 0080/0187: training loss 0.493
Epoch 95 iteration 0100/0187: training loss 0.494
Epoch 95 iteration 0120/0187: training loss 0.489
Epoch 95 iteration 0140/0187: training loss 0.487
Epoch 95 iteration 0160/0187: training loss 0.489
Epoch 95 iteration 0180/0187: training loss 0.487
Epoch 95 validation pixAcc: 0.885, mIoU: 0.430
Epoch 96 iteration 0020/0187: training loss 0.486
Epoch 96 iteration 0040/0187: training loss 0.496
Epoch 96 iteration 0060/0187: training loss 0.490
Epoch 96 iteration 0080/0187: training loss 0.488
Epoch 96 iteration 0100/0188: training loss 0.485
Epoch 96 iteration 0120/0188: training loss 0.488
Epoch 96 iteration 0140/0188: training loss 0.490
Epoch 96 iteration 0160/0188: training loss 0.490
Epoch 96 iteration 0180/0188: training loss 0.491
Epoch 96 validation pixAcc: 0.885, mIoU: 0.431
Epoch 97 iteration 0020/0187: training loss 0.467
Epoch 97 iteration 0040/0187: training loss 0.486
Epoch 97 iteration 0060/0187: training loss 0.489
Epoch 97 iteration 0080/0187: training loss 0.491
Epoch 97 iteration 0100/0187: training loss 0.490
Epoch 97 iteration 0120/0187: training loss 0.488
Epoch 97 iteration 0140/0187: training loss 0.489
Epoch 97 iteration 0160/0187: training loss 0.491
Epoch 97 iteration 0180/0187: training loss 0.490
Epoch 97 validation pixAcc: 0.886, mIoU: 0.430
Epoch 98 iteration 0020/0187: training loss 0.462
Epoch 98 iteration 0040/0187: training loss 0.465
Epoch 98 iteration 0060/0187: training loss 0.480
Epoch 98 iteration 0080/0187: training loss 0.479
Epoch 98 iteration 0100/0188: training loss 0.478
Epoch 98 iteration 0120/0188: training loss 0.481
Epoch 98 iteration 0140/0188: training loss 0.487
Epoch 98 iteration 0160/0188: training loss 0.490
Epoch 98 iteration 0180/0188: training loss 0.488
Epoch 98 validation pixAcc: 0.885, mIoU: 0.431
Epoch 99 iteration 0020/0187: training loss 0.467
Epoch 99 iteration 0040/0187: training loss 0.475
Epoch 99 iteration 0060/0187: training loss 0.482
Epoch 99 iteration 0080/0187: training loss 0.477
Epoch 99 iteration 0100/0187: training loss 0.479
Epoch 99 iteration 0120/0187: training loss 0.479
Epoch 99 iteration 0140/0187: training loss 0.482
Epoch 99 iteration 0160/0187: training loss 0.486
Epoch 99 iteration 0180/0187: training loss 0.488
Epoch 99 validation pixAcc: 0.885, mIoU: 0.432
Epoch 100 iteration 0020/0187: training loss 0.485
Epoch 100 iteration 0040/0187: training loss 0.471
Epoch 100 iteration 0060/0187: training loss 0.479
Epoch 100 iteration 0080/0187: training loss 0.480
Epoch 100 iteration 0100/0188: training loss 0.485
Epoch 100 iteration 0120/0188: training loss 0.482
Epoch 100 iteration 0140/0188: training loss 0.483
Epoch 100 iteration 0160/0188: training loss 0.481
Epoch 100 iteration 0180/0188: training loss 0.480
Epoch 100 validation pixAcc: 0.884, mIoU: 0.431
Epoch 101 iteration 0020/0187: training loss 0.495
Epoch 101 iteration 0040/0187: training loss 0.495
Epoch 101 iteration 0060/0187: training loss 0.481
Epoch 101 iteration 0080/0187: training loss 0.480
Epoch 101 iteration 0100/0187: training loss 0.481
Epoch 101 iteration 0120/0187: training loss 0.481
Epoch 101 iteration 0140/0187: training loss 0.485
Epoch 101 iteration 0160/0187: training loss 0.485
Epoch 101 iteration 0180/0187: training loss 0.485
Epoch 101 validation pixAcc: 0.885, mIoU: 0.428
Epoch 102 iteration 0020/0187: training loss 0.495
Epoch 102 iteration 0040/0187: training loss 0.476
Epoch 102 iteration 0060/0187: training loss 0.479
Epoch 102 iteration 0080/0187: training loss 0.476
Epoch 102 iteration 0100/0188: training loss 0.480
Epoch 102 iteration 0120/0188: training loss 0.480
Epoch 102 iteration 0140/0188: training loss 0.483
Epoch 102 iteration 0160/0188: training loss 0.483
Epoch 102 iteration 0180/0188: training loss 0.480
Epoch 102 validation pixAcc: 0.885, mIoU: 0.431
Epoch 103 iteration 0020/0187: training loss 0.471
Epoch 103 iteration 0040/0187: training loss 0.478
Epoch 103 iteration 0060/0187: training loss 0.472
Epoch 103 iteration 0080/0187: training loss 0.474
Epoch 103 iteration 0100/0187: training loss 0.480
Epoch 103 iteration 0120/0187: training loss 0.480
Epoch 103 iteration 0140/0187: training loss 0.478
Epoch 103 iteration 0160/0187: training loss 0.478
Epoch 103 iteration 0180/0187: training loss 0.479
Epoch 103 validation pixAcc: 0.887, mIoU: 0.436
Epoch 104 iteration 0020/0187: training loss 0.481
Epoch 104 iteration 0040/0187: training loss 0.488
Epoch 104 iteration 0060/0187: training loss 0.489
Epoch 104 iteration 0080/0187: training loss 0.487
Epoch 104 iteration 0100/0188: training loss 0.485
Epoch 104 iteration 0120/0188: training loss 0.481
Epoch 104 iteration 0140/0188: training loss 0.482
Epoch 104 iteration 0160/0188: training loss 0.480
Epoch 104 iteration 0180/0188: training loss 0.479
Epoch 104 validation pixAcc: 0.886, mIoU: 0.432
Epoch 105 iteration 0020/0187: training loss 0.460
Epoch 105 iteration 0040/0187: training loss 0.460
Epoch 105 iteration 0060/0187: training loss 0.464
Epoch 105 iteration 0080/0187: training loss 0.465
Epoch 105 iteration 0100/0187: training loss 0.471
Epoch 105 iteration 0120/0187: training loss 0.475
Epoch 105 iteration 0140/0187: training loss 0.475
Epoch 105 iteration 0160/0187: training loss 0.476
Epoch 105 iteration 0180/0187: training loss 0.473
Epoch 105 validation pixAcc: 0.886, mIoU: 0.434
Epoch 106 iteration 0020/0187: training loss 0.475
Epoch 106 iteration 0040/0187: training loss 0.473
Epoch 106 iteration 0060/0187: training loss 0.471
Epoch 106 iteration 0080/0187: training loss 0.474
Epoch 106 iteration 0100/0188: training loss 0.474
Epoch 106 iteration 0120/0188: training loss 0.475
Epoch 106 iteration 0140/0188: training loss 0.474
Epoch 106 iteration 0160/0188: training loss 0.474
Epoch 106 iteration 0180/0188: training loss 0.474
Epoch 106 validation pixAcc: 0.887, mIoU: 0.435
Epoch 107 iteration 0020/0187: training loss 0.457
Epoch 107 iteration 0040/0187: training loss 0.473
Epoch 107 iteration 0060/0187: training loss 0.467
Epoch 107 iteration 0080/0187: training loss 0.473
Epoch 107 iteration 0100/0187: training loss 0.475
Epoch 107 iteration 0120/0187: training loss 0.476
Epoch 107 iteration 0140/0187: training loss 0.473
Epoch 107 iteration 0160/0187: training loss 0.476
Epoch 107 iteration 0180/0187: training loss 0.475
Epoch 107 validation pixAcc: 0.885, mIoU: 0.432
Epoch 108 iteration 0020/0187: training loss 0.460
Epoch 108 iteration 0040/0187: training loss 0.471
Epoch 108 iteration 0060/0187: training loss 0.478
Epoch 108 iteration 0080/0187: training loss 0.476
Epoch 108 iteration 0100/0188: training loss 0.477
Epoch 108 iteration 0120/0188: training loss 0.473
Epoch 108 iteration 0140/0188: training loss 0.472
Epoch 108 iteration 0160/0188: training loss 0.473
Epoch 108 iteration 0180/0188: training loss 0.474
Epoch 108 validation pixAcc: 0.885, mIoU: 0.429
Epoch 109 iteration 0020/0187: training loss 0.469
Epoch 109 iteration 0040/0187: training loss 0.462
Epoch 109 iteration 0060/0187: training loss 0.460
Epoch 109 iteration 0080/0187: training loss 0.466
Epoch 109 iteration 0100/0187: training loss 0.469
Epoch 109 iteration 0120/0187: training loss 0.471
Epoch 109 iteration 0140/0187: training loss 0.470
Epoch 109 iteration 0160/0187: training loss 0.470
Epoch 109 iteration 0180/0187: training loss 0.471
Epoch 109 validation pixAcc: 0.886, mIoU: 0.436
Epoch 110 iteration 0020/0187: training loss 0.448
Epoch 110 iteration 0040/0187: training loss 0.458
Epoch 110 iteration 0060/0187: training loss 0.466
Epoch 110 iteration 0080/0187: training loss 0.469
Epoch 110 iteration 0100/0188: training loss 0.470
Epoch 110 iteration 0120/0188: training loss 0.467
Epoch 110 iteration 0140/0188: training loss 0.468
Epoch 110 iteration 0160/0188: training loss 0.468
Epoch 110 iteration 0180/0188: training loss 0.466
Epoch 110 validation pixAcc: 0.886, mIoU: 0.434
Epoch 111 iteration 0020/0187: training loss 0.455
Epoch 111 iteration 0040/0187: training loss 0.451
Epoch 111 iteration 0060/0187: training loss 0.464
Epoch 111 iteration 0080/0187: training loss 0.471
Epoch 111 iteration 0100/0187: training loss 0.470
Epoch 111 iteration 0120/0187: training loss 0.468
Epoch 111 iteration 0140/0187: training loss 0.469
Epoch 111 iteration 0160/0187: training loss 0.469
Epoch 111 iteration 0180/0187: training loss 0.470
Epoch 111 validation pixAcc: 0.885, mIoU: 0.433
Epoch 112 iteration 0020/0187: training loss 0.471
Epoch 112 iteration 0040/0187: training loss 0.471
Epoch 112 iteration 0060/0187: training loss 0.470
Epoch 112 iteration 0080/0187: training loss 0.472
Epoch 112 iteration 0100/0188: training loss 0.473
Epoch 112 iteration 0120/0188: training loss 0.472
Epoch 112 iteration 0140/0188: training loss 0.472
Epoch 112 iteration 0160/0188: training loss 0.471
Epoch 112 iteration 0180/0188: training loss 0.471
Epoch 112 validation pixAcc: 0.885, mIoU: 0.430
Epoch 113 iteration 0020/0187: training loss 0.455
Epoch 113 iteration 0040/0187: training loss 0.464
Epoch 113 iteration 0060/0187: training loss 0.464
Epoch 113 iteration 0080/0187: training loss 0.465
Epoch 113 iteration 0100/0187: training loss 0.466
Epoch 113 iteration 0120/0187: training loss 0.467
Epoch 113 iteration 0140/0187: training loss 0.466
Epoch 113 iteration 0160/0187: training loss 0.467
Epoch 113 iteration 0180/0187: training loss 0.465
Epoch 113 validation pixAcc: 0.885, mIoU: 0.430
Epoch 114 iteration 0020/0187: training loss 0.463
Epoch 114 iteration 0040/0187: training loss 0.463
Epoch 114 iteration 0060/0187: training loss 0.457
Epoch 114 iteration 0080/0187: training loss 0.462
Epoch 114 iteration 0100/0188: training loss 0.460
Epoch 114 iteration 0120/0188: training loss 0.465
Epoch 114 iteration 0140/0188: training loss 0.465
Epoch 114 iteration 0160/0188: training loss 0.465
Epoch 114 iteration 0180/0188: training loss 0.465
Epoch 114 validation pixAcc: 0.885, mIoU: 0.431
Epoch 115 iteration 0020/0187: training loss 0.473
Epoch 115 iteration 0040/0187: training loss 0.474
Epoch 115 iteration 0060/0187: training loss 0.468
Epoch 115 iteration 0080/0187: training loss 0.472
Epoch 115 iteration 0100/0187: training loss 0.472
Epoch 115 iteration 0120/0187: training loss 0.472
Epoch 115 iteration 0140/0187: training loss 0.472
Epoch 115 iteration 0160/0187: training loss 0.469
Epoch 115 iteration 0180/0187: training loss 0.466
Epoch 115 validation pixAcc: 0.886, mIoU: 0.431
Epoch 116 iteration 0020/0187: training loss 0.488
Epoch 116 iteration 0040/0187: training loss 0.476
Epoch 116 iteration 0060/0187: training loss 0.479
Epoch 116 iteration 0080/0187: training loss 0.477
Epoch 116 iteration 0100/0188: training loss 0.475
Epoch 116 iteration 0120/0188: training loss 0.472
Epoch 116 iteration 0140/0188: training loss 0.469
Epoch 116 iteration 0160/0188: training loss 0.469
Epoch 116 iteration 0180/0188: training loss 0.466
Epoch 116 validation pixAcc: 0.885, mIoU: 0.432
Epoch 117 iteration 0020/0187: training loss 0.454
Epoch 117 iteration 0040/0187: training loss 0.452
Epoch 117 iteration 0060/0187: training loss 0.457
Epoch 117 iteration 0080/0187: training loss 0.460
Epoch 117 iteration 0100/0187: training loss 0.462
Epoch 117 iteration 0120/0187: training loss 0.466
Epoch 117 iteration 0140/0187: training loss 0.462
Epoch 117 iteration 0160/0187: training loss 0.460
Epoch 117 iteration 0180/0187: training loss 0.462
Epoch 117 validation pixAcc: 0.886, mIoU: 0.432
Epoch 118 iteration 0020/0187: training loss 0.463
Epoch 118 iteration 0040/0187: training loss 0.473
Epoch 118 iteration 0060/0187: training loss 0.463
Epoch 118 iteration 0080/0187: training loss 0.459
Epoch 118 iteration 0100/0188: training loss 0.460
Epoch 118 iteration 0120/0188: training loss 0.463
Epoch 118 iteration 0140/0188: training loss 0.465
Epoch 118 iteration 0160/0188: training loss 0.466
Epoch 118 iteration 0180/0188: training loss 0.467
Epoch 118 validation pixAcc: 0.885, mIoU: 0.430
Epoch 119 iteration 0020/0187: training loss 0.463
Epoch 119 iteration 0040/0187: training loss 0.473
Epoch 119 iteration 0060/0187: training loss 0.467
Epoch 119 iteration 0080/0187: training loss 0.465
Epoch 119 iteration 0100/0187: training loss 0.468
Epoch 119 iteration 0120/0187: training loss 0.464
Epoch 119 iteration 0140/0187: training loss 0.463
Epoch 119 iteration 0160/0187: training loss 0.465
Epoch 119 iteration 0180/0187: training loss 0.466
Epoch 119 validation pixAcc: 0.885, mIoU: 0.430
